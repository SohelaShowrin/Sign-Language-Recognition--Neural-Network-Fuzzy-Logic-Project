# Sign-Language-Recognition--Neural-Network-Fuzzy-Logic-Project

# üè∑INTRODUCTION
 Sign language is an essential communication method
 for individuals with hearing impairments, enabling them to
 express thoughts, emotions, and needs effectively. However, a
 significant communication gap exists between sign language
 users and non-users. This paper presents a deep learning
based system that converts American Sign Language (ASL)
 gestures into corresponding letters using a highly optimized
 Convolutional Neural Network (CNN). The model is trained on
 a dataset containing 26 classes (A-Z) and achieves 99.03% of
 training accuracy and 90.03% testing accuracy, demonstrating
 its near-perfect precision in recognizing ASL hand gestures.
 The system preprocesses images, applies normalization, and
 performs translation, making communication more accessible.
 By optimizing hyperparameters and implementing batch nor
malization, dropout techniques ,one hot encoding, the model
 ensures high generalization across various hand gestures. This
 approach enhances daily interactions for individuals with hearing
 impairments and promotes inclusivity in social, educational, and
 professional settings. The project highlights the effectiveness of
 deep learning in bridging communication barriers and improving
 accessibility through technology.

 # üîëResults
                                                       
<img width="497" height="457" alt="output" src="https://github.com/user-attachments/assets/0027e7ad-2ec4-4e3e-a801-904111bb5c6f" />

# ‚úÖCONCLUSION
This project shows how important it is to model sign
 language gestures accurately using Convolutional Neural Net
works (CNNs). By using the American Sign Language (ASL)
 dataset, our improved system translates ASL gestures more
 consistently, helping people who don‚Äôt know sign language
 communicate better with those who do. This is especially
 helpful for people with hearing impairments, making it easier
 for them to interact in schools, workplaces, and daily life.
 Our work also demonstrates how deep learning can solve
 real-world problems and make technology more inclusive. In
 the future, we plan to test the system with different types of
 gestures and improve how quickly and accurately it recognizes
 them in various situations.
